import torch
import logging
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from stable_cot_module import LightweightCoTModule
from dimension_utils import check_tensor_dimensions, fix_batch_dimensions, safe_tensor_cat
from device_utils import check_batch_device_consistency, fix_batch_device_consistency
from configs import cfg

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    logging.info("=== æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ ===")
    
    try:
        # 1. åˆ›å»ºCoTæ¨¡å—
        cot_module = LightweightCoTModule(cfg)
        logging.info("âœ“ CoTæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
        # 2. åˆ›å»ºæ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
        batch = {
            'image_features': torch.randn(2, 512),
            'text_features': torch.randn(2, 512),
            'labels': torch.randint(0, 2, (2,)),
            'image_descriptions': ['A person in an image', 'Another person in image'],
            'text_contents': ['Sample text 1', 'Sample text 2']
        }
        logging.info("âœ“ æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®åˆ›å»ºæˆåŠŸ")
        
        # 3. æ£€æŸ¥æ‰¹æ¬¡ç»´åº¦
        logging.info("åŸå§‹æ‰¹æ¬¡:")
        check_tensor_dimensions(batch, "åŸå§‹æ‰¹æ¬¡")
        
        # 4. ä¿®å¤æ‰¹æ¬¡ç»´åº¦
        fixed_batch = fix_batch_dimensions(batch)
        logging.info("ä¿®å¤åæ‰¹æ¬¡:")
        check_tensor_dimensions(fixed_batch, "ä¿®å¤åæ‰¹æ¬¡")
        
        # 5. æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
        if check_batch_device_consistency(fixed_batch):
            logging.info("âœ“ æ‰¹æ¬¡è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
        else:
            logging.warning("âš  æ‰¹æ¬¡è®¾å¤‡ä¸ä¸€è‡´ï¼Œæ­£åœ¨ä¿®å¤...")
            target_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            fixed_batch = fix_batch_device_consistency(fixed_batch, target_device)
        
        # 6. æµ‹è¯•CoTæ¨ç†
        reasoning_features_list = []
        reasoning_texts = []
        
        for i in range(len(fixed_batch['image_features'])):
            img_feat = fixed_batch['image_features'][i:i+1]
            txt_feat = fixed_batch['text_features'][i:i+1]
            img_desc = fixed_batch['image_descriptions'][i]
            txt_content = fixed_batch['text_contents'][i]
            
            with torch.no_grad():
                reasoning_feat, reasoning_text = cot_module(
                    img_feat, txt_feat, img_desc, txt_content, "hate"
                )
            
            reasoning_features_list.append(reasoning_feat)
            reasoning_texts.append(reasoning_text)
        
        # 7. è¿æ¥æ¨ç†ç‰¹å¾
        reasoning_features = safe_tensor_cat(reasoning_features_list, dim=0)
        logging.info(f"âœ“ æ¨ç†ç‰¹å¾è¿æ¥æˆåŠŸ: {reasoning_features.shape}")
        
        # 8. æ¨¡æ‹Ÿåˆ†ç±»å™¨è¾“å‡º
        original_features = torch.randn(2, 1024)  # æ¨¡æ‹ŸåŸå§‹ç‰¹å¾
        combined_features = torch.cat([original_features, reasoning_features], dim=1)
        logging.info(f"âœ“ ç‰¹å¾èåˆæˆåŠŸ: {combined_features.shape}")
        
        # 9. æ¨¡æ‹Ÿåˆ†ç±»
        logits = torch.randn(2, 2)  # æ¨¡æ‹Ÿåˆ†ç±»å™¨è¾“å‡º
        predictions = torch.argmax(logits, dim=1)
        logging.info(f"âœ“ åˆ†ç±»å®Œæˆ: {predictions}")
        
        logging.info("ğŸ‰ å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        logging.error(f"âŒ å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    logging.info("=== æµ‹è¯•é”™è¯¯å¤„ç† ===")
    
    try:
        # æµ‹è¯•ç©ºå¼ é‡åˆ—è¡¨
        try:
            result = safe_tensor_cat([], dim=0)
            logging.error("âŒ åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
            return False
        except ValueError:
            logging.info("âœ“ ç©ºå¼ é‡åˆ—è¡¨æ­£ç¡®å¤„ç†")
        
        # æµ‹è¯•è®¾å¤‡ä¸åŒ¹é…
        tensor1 = torch.randn(2, 3)
        tensor2 = torch.randn(2, 3)
        if torch.cuda.is_available():
            tensor2 = tensor2.cuda()
        
        try:
            result = safe_tensor_cat([tensor1, tensor2], dim=0)
            logging.info("âœ“ è®¾å¤‡ä¸åŒ¹é…è‡ªåŠ¨ä¿®å¤")
        except Exception as e:
            logging.error(f"âŒ è®¾å¤‡ä¸åŒ¹é…å¤„ç†å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•ç»´åº¦ä¸åŒ¹é…
        tensor1 = torch.randn(2, 3)
        tensor2 = torch.randn(3)  # 1Då¼ é‡
        
        try:
            result = safe_tensor_cat([tensor1, tensor2.unsqueeze(0)], dim=0)
            logging.info("âœ“ ç»´åº¦ä¸åŒ¹é…è‡ªåŠ¨ä¿®å¤")
        except Exception as e:
            logging.error(f"âŒ ç»´åº¦ä¸åŒ¹é…å¤„ç†å¤±è´¥: {e}")
            return False
        
        logging.info("ğŸ‰ é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        logging.error(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    logging.info("=== æµ‹è¯•æ€§èƒ½ ===")
    
    try:
        import time
        
        # åˆ›å»ºCoTæ¨¡å—
        cot_module = LightweightCoTModule(cfg)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 10
        test_image_features = torch.randn(batch_size, 512)
        test_text_features = torch.randn(batch_size, 512)
        test_descriptions = [f"Description {i}" for i in range(batch_size)]
        test_contents = [f"Content {i}" for i in range(batch_size)]
        
        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        start_time = time.time()
        
        with torch.no_grad():
            for i in range(batch_size):
                reasoning_feat, reasoning_text = cot_module(
                    test_image_features[i:i+1],
                    test_text_features[i:i+1],
                    test_descriptions[i],
                    test_contents[i],
                    "hate"
                )
        
        end_time = time.time()
        inference_time = end_time - start_time
        
        logging.info(f"âœ“ æ¨ç†é€Ÿåº¦: {inference_time:.4f}ç§’ ({batch_size}ä¸ªæ ·æœ¬)")
        logging.info(f"âœ“ å¹³å‡æ¨ç†æ—¶é—´: {inference_time/batch_size:.4f}ç§’/æ ·æœ¬")
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        import psutil
        process = psutil.Process()
        memory_usage = process.memory_info().rss / 1024 / 1024  # MB
        logging.info(f"âœ“ å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
        
        logging.info("ğŸ‰ æ€§èƒ½æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        logging.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logging.info("ğŸš€ å¼€å§‹é›†æˆæµ‹è¯•...")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_complete_pipeline,
        test_error_handling,
        test_performance
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            logging.error(f"æµ‹è¯• {test.__name__} å¤±è´¥: {e}")
            results.append(False)
    
    # æ€»ç»“ç»“æœ
    passed = sum(results)
    total = len(results)
    logging.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        logging.info("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡! å¯ä»¥å¼€å§‹è®­ç»ƒäº†!")
        return True
    else:
        logging.error("âŒ éƒ¨åˆ†é›†æˆæµ‹è¯•å¤±è´¥! è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1) 